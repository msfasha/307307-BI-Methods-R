{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a structured lecture on **Neural Networks**, covering their **features, history, perceptron, backpropagation, deep learning architectures (DNNs, CNNs, RNNs, LSTMs, etc.), and additional useful information**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lecture: Introduction to Neural Networks**\n",
    "\n",
    "### **1. Introduction**\n",
    "Neural Networks (NNs) are a subset of machine learning, inspired by the human brain. They are designed to recognize patterns and relationships in data through interconnected layers of artificial neurons.\n",
    "\n",
    "### **2. Features of Neural Networks**\n",
    "- **Non-linearity**: Neural networks can model complex, non-linear relationships in data.\n",
    "- **Adaptive Learning**: They adjust weights based on training data using optimization techniques.\n",
    "- **Generalization**: With proper training, NNs can make accurate predictions on unseen data.\n",
    "- **Parallel Computation**: Due to multiple neurons working simultaneously, they can handle large datasets efficiently.\n",
    "- **Feature Extraction**: They automatically learn hierarchical features without manual engineering.\n",
    "\n",
    "### **3. A Brief History of Neural Networks**\n",
    "1. **1943** â€“ McCulloch & Pitts proposed the first artificial neuron.\n",
    "2. **1958** â€“ Rosenblatt introduced the **Perceptron** (first trainable neural network).\n",
    "3. **1970s-1980s** â€“ Introduction of **Backpropagation**, making deeper networks feasible.\n",
    "4. **1990s** â€“ **Support Vector Machines (SVMs)** and other ML algorithms overshadowed NNs due to computational limits.\n",
    "5. **2006-Present** â€“ **Deep Learning Revolution** (Hinton & Bengio's breakthroughs in deep networks, CNNs, RNNs, LSTMs).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Perceptron: The Fundamental Unit of Neural Networks**\n",
    "The **Perceptron** is the simplest form of a neural network, functioning as a linear classifier.\n",
    "\n",
    "### **Mathematical Representation**\n",
    "A perceptron takes multiple inputs, applies weights, sums them up, and passes them through an activation function:\n",
    "\\[\n",
    "y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n",
    "\\]\n",
    "Where:\n",
    "- \\( x_i \\) = input features\n",
    "- \\( w_i \\) = weights\n",
    "- \\( b \\) = bias\n",
    "- \\( f(x) \\) = activation function (e.g., step function, sigmoid)\n",
    "\n",
    "### **Code Example: Single-Layer Perceptron**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, epochs=100):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0  # Step function\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(n_samples):\n",
    "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
    "                y_predicted = self.activation(linear_output)\n",
    "                update = self.lr * (y[i] - y_predicted)\n",
    "                self.weights += update * X[i]\n",
    "                self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return np.array([self.activation(x) for x in linear_output])\n",
    "\n",
    "# Example Usage:\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])  # AND gate inputs\n",
    "y = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "\n",
    "model = Perceptron(learning_rate=0.1, epochs=10)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Backpropagation: Training Multi-Layer Neural Networks**\n",
    "Backpropagation enables multi-layer networks by efficiently computing gradients using the **chain rule**.\n",
    "\n",
    "### **Steps in Backpropagation**\n",
    "1. **Forward Pass**: Compute outputs from input to output layer.\n",
    "2. **Loss Computation**: Measure error using a loss function (e.g., MSE, cross-entropy).\n",
    "3. **Backward Pass**:\n",
    "   - Compute **gradients** using the chain rule.\n",
    "   - Update weights using **Gradient Descent**.\n",
    "\n",
    "### **Code Example: Multi-Layer Perceptron (MLP) with Backpropagation**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)  # Input to hidden\n",
    "        self.fc2 = nn.Linear(3, 1)  # Hidden to output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Data (AND gate)\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)\n",
    "\n",
    "# Training\n",
    "model = NeuralNet()\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predictions\n",
    "print(model(X).detach().numpy())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Deep Neural Networks (DNNs)**\n",
    "- **More hidden layers** â†’ Better ability to extract complex patterns.\n",
    "- Uses activation functions like **ReLU**, **Sigmoid**, and **Softmax**.\n",
    "- Can suffer from **vanishing gradients**, solved using **batch normalization**, **ReLU**, and **residual connections**.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Convolutional Neural Networks (CNNs)**\n",
    "- Designed for **image processing**.\n",
    "- Uses **convolutional layers**, **pooling layers**, and **fully connected layers**.\n",
    "- Applications: **Object detection, facial recognition, medical imaging**.\n",
    "\n",
    "**Example Layers in CNN:**\n",
    "1. **Conv Layer**: Extracts features using filters.\n",
    "2. **Pooling Layer**: Reduces dimensions, making computation efficient.\n",
    "3. **Fully Connected Layer**: Outputs predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Recurrent Neural Networks (RNNs)**\n",
    "- Designed for **sequential data** (e.g., time-series, text).\n",
    "- Has **hidden states** that retain previous information.\n",
    "- Suffers from **vanishing gradients**, leading to **LSTMs** and **GRUs**.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Long Short-Term Memory (LSTMs)**\n",
    "- Improves RNNs by handling **long-term dependencies**.\n",
    "- Uses **gates (input, forget, output)** to control information flow.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Transformer Networks (BERT, GPT)**\n",
    "- **Self-attention** mechanism allows parallel processing.\n",
    "- Powers **ChatGPT, Google BERT, OpenAIâ€™s GPT models**.\n",
    "- State-of-the-art in **language modeling, translation, chatbots**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "- **Neural networks** are powerful tools for diverse applications.\n",
    "- **Backpropagation** allows training deep models.\n",
    "- **CNNs** specialize in image recognition.\n",
    "- **RNNs, LSTMs** are best for sequential data.\n",
    "- **Transformers** dominate NLP applications.\n",
    "\n",
    "This lecture provides a solid foundation in **Neural Networks**, from theory to implementation. Would you like a specific case study or dataset for hands-on practice? ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
