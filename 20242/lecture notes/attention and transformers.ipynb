{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lecture: Attention Mechanisms and Transformer Networks**\n",
    "### **1. Introduction**\n",
    "In recent years, **transformer networks** have revolutionized natural language processing (NLP) and machine learning. The **attention mechanism**, introduced in 2014, became a cornerstone of these models, enabling them to process long sequences efficiently. This lecture covers:\n",
    "\n",
    "- The evolution of attention mechanisms\n",
    "- The structure of transformer networks\n",
    "- Practical examples of their implementation\n",
    "\n",
    "---\n",
    "\n",
    "### **2. History of Attention Mechanisms**\n",
    "Before transformers, recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) were the dominant architectures for sequential data, but they had limitations:\n",
    "\n",
    "- **Limited memory:** RNNs struggle with long-range dependencies due to vanishing gradients.\n",
    "- **Sequential computation:** Training and inference were slow because they process tokens one by one.\n",
    "\n",
    "To address these issues:\n",
    "- **2014:** Bahdanau et al. introduced the **attention mechanism** for machine translation, allowing the model to \"focus\" on relevant parts of the input sequence.\n",
    "- **2017:** Vaswani et al. introduced the **Transformer** architecture in the paper *\"Attention is All You Need,\"* which entirely replaced RNNs with self-attention layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Understanding the Attention Mechanism**\n",
    "#### **3.1. Key Components of Attention**\n",
    "Attention mechanisms compute the importance of each input element. Given a sequence, the attention mechanism calculates:\n",
    "\n",
    "- **Query (Q):** Represents the current word being processed.\n",
    "- **Key (K):** Represents all words in the sequence.\n",
    "- **Value (V):** Represents the actual content of words.\n",
    "\n",
    "The attention score is computed using the **scaled dot-product attention formula**:\n",
    "\n",
    "$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$\n",
    "\n",
    "where \\( d_k \\) is the dimension of the keys.\n",
    "\n",
    "#### **3.2. Example: Implementing Scaled Dot-Product Attention in Python**\n",
    "Here's a simple implementation using NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = K.shape[-1]  # Key dimension\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Compute attention scores\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Softmax\n",
    "    return np.dot(attention_weights, V)\n",
    "\n",
    "# Example inputs\n",
    "Q = np.array([[1, 0, 1]])  # Query\n",
    "K = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])  # Keys\n",
    "V = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  # Values\n",
    "\n",
    "output = scaled_dot_product_attention(Q, K, V)\n",
    "print(output)\n",
    "```\n",
    "This function computes the attention-weighted sum of values.\n",
    "\n",
    "### **4. The Transformer Architecture**\n",
    "A transformer consists of **an encoder-decoder structure** with multiple layers of attention.\n",
    "\n",
    "#### **4.1. Transformer Components**\n",
    "- **Self-Attention Mechanism:** Allows the model to focus on different words at different times.\n",
    "- **Multi-Head Attention:** Uses multiple attention heads to capture diverse relationships.\n",
    "- **Feedforward Layers:** Position-wise fully connected layers add non-linearity.\n",
    "- **Positional Encoding:** Since transformers donâ€™t process sequences sequentially, positional encodings help encode word order.\n",
    "\n",
    "#### **4.2. Multi-Head Attention**\n",
    "Multi-head attention applies attention multiple times in parallel, concatenates the results, and projects them:\n",
    "\n",
    "$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$\n",
    "\n",
    "where each head has independent \\( W^Q, W^K, W^V \\) matrices.\n",
    "\n",
    "### **5. Implementing a Transformer Model with PyTorch**\n",
    "Now, let's implement a simple transformer block using PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(SimpleTransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        attention_out, _ = self.attention(query, key, value)\n",
    "        x = self.dropout(self.norm1(attention_out + query))\n",
    "        forward_out = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward_out + x))\n",
    "        return out\n",
    "\n",
    "# Example input\n",
    "embed_size = 128\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "forward_expansion = 4\n",
    "\n",
    "transformer_block = SimpleTransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "query = key = value = torch.rand(10, 1, embed_size)  # 10 tokens, batch size 1\n",
    "\n",
    "output = transformer_block(value, key, query)\n",
    "print(output.shape)  # Output shape should be (10, 1, embed_size)\n",
    "```\n",
    "\n",
    "### **6. Applications of Transformers**\n",
    "- **Machine Translation (e.g., Google Translate)**\n",
    "- **Text Generation (e.g., GPT models)**\n",
    "- **Speech Recognition (e.g., Whisper)**\n",
    "- **Computer Vision (e.g., Vision Transformers - ViTs)**\n",
    "- **Reinforcement Learning (e.g., Decision Transformers)**\n",
    "\n",
    "### **7. Implementing a Pretrained Transformer (BERT)**\n",
    "Let's use Hugging Face's `transformers` library to load a pretrained **BERT model** for text classification:\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenizing input text\n",
    "text = \"Transformers are powerful!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)  # Logits for classification\n",
    "```\n",
    "\n",
    "### **8. Summary**\n",
    "- **Attention mechanisms** allow models to focus on relevant information.\n",
    "- **Transformers replace RNNs** with self-attention layers, enabling parallel processing.\n",
    "- **Multi-head attention** helps the model capture diverse relationships.\n",
    "- **Pretrained models like BERT and GPT** leverage transformers for NLP applications.\n",
    "\n",
    "### **9. Conclusion**\n",
    "Transformers have become the backbone of modern AI models, surpassing older architectures in efficiency and effectiveness. Future research explores **sparse attention** and **efficient transformers** to improve scalability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
