{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of splitting raw text (e.g., a sentence) into smaller units called **tokens**, which serve as the basic inputs to a language model. These tokens can range from full words to subwords, characters, or even byte-level segments, depending on the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why It Matters\n",
    "\n",
    "- **Bridging Text and Model**: Large language models operate on sequences of token IDs (integers). Converting raw text to tokens is the first step in converting text into a numeric representation that can be processed by the model.\n",
    "- **Vocabulary Management**: A tokenizer defines a vocabulary of all possible tokens. Keeping the vocabulary size balanced is crucial—too large increases model memory usage, and too small might lead to inefficiency or hamper the model’s ability to represent rare words accurately.\n",
    "- **Efficiency and Accuracy**: Effective tokenization can help capture the semantic meaning of subwords and handle rare or out-of-vocabulary words gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Tokenization\n",
    "\n",
    "### 2.1 Whitespace and Rule-Based Tokenization\n",
    "- **Description**: The simplest form, splitting text on whitespace or punctuation. \n",
    "- **Example**: “Hello, world!” → [“Hello,”, “world!”]\n",
    "- **Drawbacks**: Not sophisticated enough for languages without whitespace delimiters (e.g., Chinese) or for morphological variations. It also leads to large vocabularies since each unique form of a word is treated as a separate token.\n",
    "\n",
    "### 2.2 Subword Tokenization\n",
    "Subword tokenization splits words into smaller units based on frequency statistics from the training corpus. This way, frequent words remain as single tokens, while rare words are broken into subwords.\n",
    "\n",
    "Common subword algorithms:\n",
    "1. **Byte Pair Encoding (BPE)**\n",
    "   - Iteratively merges the most frequent pairs of characters (or character sequences) into a single token.\n",
    "   - Example:\n",
    "     - Start: “l”, “o”, “w”, “er”, “ing”, ...\n",
    "     - Merge frequently co-occurring pairs: “lo” → “low”, etc.\n",
    "   - Used by GPT-2 and many other models.\n",
    "\n",
    "2. **WordPiece**\n",
    "   - Similar to BPE but uses a slightly different algorithm for merging subwords based on likelihood. \n",
    "   - Used by BERT.\n",
    "\n",
    "3. **Unigram**\n",
    "   - Starts with a large vocabulary and iteratively removes tokens that have the smallest impact on the model’s overall likelihood. \n",
    "   - Used by SentencePiece, often in models like RoBERTa, XLNet, and T5.\n",
    "\n",
    "### 2.3 Character-Level or Byte-Level Tokenization\n",
    "- **Description**: Splits text into individual characters or bytes.\n",
    "- **Use Cases**: Useful for languages with complex or large character sets, or for tasks that require analyzing text at the character level (e.g., certain speech or OCR tasks).\n",
    "- **Example**: GPT-NeoX uses GPT-2’s Byte-Level BPE, which effectively captures text at the byte level before merging subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Concepts in Tokenization\n",
    "\n",
    "1. **Vocabulary**: A mapping from tokens (subwords) to integer IDs. During training, a fixed-size vocabulary is built from the training corpus.\n",
    "2. **Special Tokens**: Tokenizers often define additional tokens for special purposes (e.g., `[CLS]` for BERT’s classification token, `[PAD]` for padding, or `<|endoftext|>` for GPT).\n",
    "3. **Out-of-Vocabulary (OOV) Handling**: With subword tokenization, truly out-of-vocabulary words are broken into known smaller subwords, drastically reducing the chance of not recognizing a word at all.\n",
    "4. **Decoding**: The inverse process of tokenization, mapping token IDs back to human-readable text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Code Examples\n",
    "\n",
    "### 4.1 Using Hugging Face Transformers (GPT-2 Tokenizer)\n",
    "\n",
    "Below is a simple example of using the GPT-2 tokenizer, which implements byte-level BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "# Encode text into token IDs\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Decode token IDs back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `encode` turns the input string into a list of integer IDs.\n",
    "- `decode` maps the list of IDs back to a string (not always identical to the original string due to how spacing is handled, but usually close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Using BERT’s WordPiece Tokenizer\n",
    "\n",
    "BERT-based models use WordPiece. Below is how to use the tokenizer from a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Tokenization helps large language models interpret input text.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(\"Encoded input:\", encoded_input)\n",
    "\n",
    "# This shows token ids and attention masks\n",
    "tokens = encoded_input['input_ids'][0]\n",
    "print(\"Token IDs:\", tokens)\n",
    "\n",
    "# Convert token IDs back to subword tokens (strings)\n",
    "token_strings = [tokenizer.convert_ids_to_tokens([tid])[0] for tid in tokens]\n",
    "print(\"Subword tokens:\", token_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `tokenizer(...)` returns a dictionary with `'input_ids'` and an `'attention_mask'`.\n",
    "- `'input_ids'` is a batch of lists of token IDs; `'attention_mask'` indicates which positions are actual text vs. padding.\n",
    "- `convert_ids_to_tokens` shows the raw subword pieces (e.g., “##ation” for “-ation” parts of a word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using SentencePiece with a Custom Tokenizer (Unigram)\n",
    "\n",
    "[SentencePiece](https://github.com/google/sentencepiece) is a library that can create subword tokenizers via Unigram or BPE. A minimal example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing sentencepiece\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Training SentencePiece tokenizer from a raw text file \n",
    "# (e.g., 'corpus.txt') with a vocabulary size of 8000\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=corpus.txt --model_prefix=m --vocab_size=8000'\n",
    ")\n",
    "\n",
    "# This creates m.model and m.vocab\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "text = \"Tokenization is vital for NLP tasks.\"\n",
    "encoded_pieces = sp.encode_as_pieces(text)\n",
    "encoded_ids = sp.encode_as_ids(text)\n",
    "\n",
    "print(\"Subword tokens:\", encoded_pieces)\n",
    "print(\"Token IDs:\", encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- You first train a SentencePiece model on your corpus (`corpus.txt`).\n",
    "- Then you load the trained model to tokenize text. \n",
    "- This approach is language-agnostic and can handle spaces by encoding them as special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Considerations\n",
    "\n",
    "1. **Vocabulary Size vs. Performance**: \n",
    "   - A larger vocabulary reduces the length of token sequences but increases memory usage and can lead to more computational overhead in the embedding layer.\n",
    "   - A smaller vocabulary leads to longer sequences but is more memory-efficient and robust for rare words.\n",
    "\n",
    "2. **Tokenization and Multilingual Models**:\n",
    "   - Multilingual models (e.g., mBERT, XLM-R) typically rely on shared subword vocabularies that work across many languages. \n",
    "   - The presence of multiple scripts introduces complexities (e.g., handling non-Latin scripts).\n",
    "\n",
    "3. **Runtime Speed**:\n",
    "   - Tokenization can be a bottleneck in large inference pipelines.\n",
    "   - Tools like Hugging Face’s fast tokenizers (written in Rust) can significantly speed up this step.\n",
    "\n",
    "4. **Impact on Downstream Tasks**:\n",
    "   - For classification tasks, adding special tokens or controlling subword splitting can change model performance.\n",
    "   - For generation tasks (like GPT-2 or GPT-3), the subword splits can affect how the model “chooses” the next token.\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "Tokenization is a **foundational step** in large language model processing. By choosing an effective tokenizer and subword method, we can ensure balanced vocabulary coverage, handle rare or unknown words gracefully, and optimize both training and inference. Whether you use BPE, WordPiece, or Unigram, the main goal is to allow the model to process text in a way that captures both semantics and morphology efficiently.\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Tokenization bridges raw text and model input format.\n",
    "- Subword tokenization (BPE, WordPiece, Unigram) dominates modern NLP because it handles rare words and varying forms well.\n",
    "- Libraries like Hugging Face Transformers and SentencePiece provide easy-to-use, industrial-strength implementations.\n",
    "\n",
    "**Further Reading & Practice**:\n",
    "- [Hugging Face Tokenizers Documentation](https://github.com/huggingface/tokenizers)\n",
    "- [Google’s SentencePiece Repository](https://github.com/google/sentencepiece)\n",
    "- Official model-specific tokenizers (e.g., GPT-2, BERT) in Hugging Face Transformers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
