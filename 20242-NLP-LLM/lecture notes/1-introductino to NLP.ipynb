{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing and Classical Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "    <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods-LLMs/blob/main/lecture%20notes/1-introductino%20to%20NLP.ipynb\" target=\"_blank\">    \n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"height: 25px; margin-right: 20px;\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By the end of this lecture, students will be able to:\n",
    "1. Understand the basic concepts and challenges of Natural Language Processing\n",
    "2. Explain key text preprocessing techniques\n",
    "3. Describe and implement statistical language models\n",
    "4. Understand n-gram models and their limitations\n",
    "5. Apply basic NLP techniques using Python libraries\n",
    "\n",
    "#### What is Natural Language Processing?\n",
    "\n",
    "Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics focused on enabling computers to understand, interpret, and generate human language. It forms the foundation for all language technologies we use today, from search engines to voice assistants and modern large language models.\n",
    "\n",
    "#### Why is NLP Challenging?\n",
    "\n",
    "Human language is:\n",
    "\n",
    "- **Ambiguous**: Words and sentences can have multiple interpretations\n",
    "  - **Lexical ambiguity**: When a word has multiple meanings\n",
    "    - Example: \"The bank is closed\" (financial institution or riverbank)\n",
    "    - Example: \"The match was struck\" (Small stick used to create fire or Competition between teams)\n",
    "    - Example: \"The suit was expensive\" (Formal set of clothes or Legal proceeding)\n",
    "    - Computational challenge: Systems must select the correct word sense from multiple possibilities\n",
    "  \n",
    "  - **Syntactic ambiguity**: When a sentence can be parsed in multiple ways\n",
    "    - Example: \"I saw the man with the telescope\" (Who has the telescope?)\n",
    "    - Example: \"Flying planes can be dangerous\" (The act of flying planes or planes that are flying?)\n",
    "    - Example: \"Time flies like an arrow\" (Multiple possible grammatical structures)\n",
    "    - Computational challenge: Models must determine the correct grammatical structure\n",
    "  \n",
    "  - **Semantic ambiguity**: When the meaning of a sentence has multiple interpretations\n",
    "    - Example: \"Every student took a different course\" (Each student took one course, and no two students took the same course? Or each student took multiple courses that differed from their own other courses?)\n",
    "    - Example: \"The chicken is ready to eat\" (Ready to be eaten or ready to consume food?)\n",
    "    - Example: \"John and Mary got married last year\" (To each other or to other people?)\n",
    "    - Computational challenge: Systems must infer the intended meaning based on context\n",
    "  \n",
    "  - **Pragmatic ambiguity**: When the intended meaning depends on context, intent, or implied information\n",
    "    - Example: \"It's cold in here\" (Statement of fact or request to close a window/turn up heat?)\n",
    "    - Example: \"Do you know what time it is?\" (Yes/no question or request for the time?)\n",
    "    - Example: \"Could you pass the salt?\" (Question about ability or request for action?)\n",
    "    - Computational challenge: Models must understand communicative intent beyond literal meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common NLP Applications\n",
    "\n",
    "1. **Sentiment analysis**: Determining the emotional tone of text\n",
    "   - Identifying if customer reviews are positive, negative, or neutral\n",
    "   - Tracking public opinion on products, services, or topics\n",
    "\n",
    "2. **Machine translation**: Translating text between languages\n",
    "   - Converting text from one language to another while preserving meaning\n",
    "   - Handling cultural and linguistic differences across languages\n",
    "\n",
    "3. **Text generation**: Creating human-like text\n",
    "   - Automatic summarization of longer documents\n",
    "   - Creative writing assistance and content creation\n",
    "   - Dialogue systems for conversational AI\n",
    "\n",
    "4. **Question answering**: Providing answers to natural language questions\n",
    "   - Extracting answers from documents or knowledge bases\n",
    "   - Understanding user intent and providing relevant information\n",
    "\n",
    "5. **Information extraction**: Identifying and extracting structured information from text\n",
    "   - Pulling key data points from unstructured documents\n",
    "   - Converting text documents into structured databases\n",
    "\n",
    "6. **Text classification**: Categorizing text into predefined categories\n",
    "   - Sorting emails into spam/not spam\n",
    "   - Organizing documents by topic or content type\n",
    "\n",
    "Understanding these techniques and applications provides the foundation for studying language models, which combine these elements to create systems that can comprehend and generate human language at increasingly sophisticated levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we need to install a text processing library i.e. NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK (Natural Language Toolkit)** is a leading platform for building Python programs that work with human language data. Developed by Steven Bird and Edward Loper at the University of Pennsylvania, NLTK has become one of the most widely used libraries for natural language processing (NLP) in Python since its initial release in 2001.\n",
    "Key Features of NLTK\n",
    "\n",
    "Comprehensive Toolkit: NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "Educational Purpose: NLTK was designed with education in mind. It comes with comprehensive documentation and a book (\"Natural Language Processing with Python\") that makes it accessible for beginners.\n",
    "Built-in Datasets: NLTK includes various pre-loaded corpora like the Brown Corpus, WordNet, and many others that are useful for training and testing NLP algorithms.\n",
    "Modular Architecture: The library is organized into modules focused on specific NLP tasks, making it easy to use just the components you need.\n",
    "\n",
    "**Core Functionality**\n",
    "NLTK supports many essential NLP tasks:\n",
    "\n",
    "- Tokenization: Breaking text into words, sentences, or other meaningful elements\n",
    "- Part-of-speech tagging: Identifying the grammatical parts of speech for words\n",
    "- Named entity recognition: Identifying names of people, organizations, locations, etc.\n",
    "- Stemming and lemmatization: Reducing words to their base or root form\n",
    "- Parsing: Analyzing syntactic structures of sentences\n",
    "- Semantic analysis: Working with meaning representations\n",
    "- Sentiment analysis: Determining positive, negative, or neutral sentiment in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install NLTK package\n",
    "! pip install nltk\n",
    "! python -m nltk.downloader all\n",
    "\n",
    "# install punkt package\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Show where nltk is looking for the data\n",
    "# print(nltk.data.path)  # Shows where NLTK is looking for data\n",
    "\n",
    "# Look for the nltk_data folder and set its path\n",
    "# nltk.data.path.append(\"C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizer\")  # Add the path to the data folder\n",
    "\n",
    "# If it fails, try to install NLTK data and everything\n",
    "# ! python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP encompasses a wide range of techniques and applications that work together to process and understand language:\n",
    "\n",
    "#### Fundamental NLP Techniques\n",
    "\n",
    "**1. Tokenization**: Breaking text into words, phrases, or other meaningful elements\n",
    "   - Word tokenization: Splitting \"I love NLP.\" into [\"I\", \"love\", \"NLP\", \".\"]\n",
    "   - Subword tokenization: Breaking words into meaningful pieces (e.g., \"playing\" → [\"play\", \"##ing\"])\n",
    "   - Character tokenization: Splitting text into individual characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/tokenization.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world Application:** A company analyzing customer support emails needs to tokenize messages to count word frequencies and identify common issues. For example, tokenizing \"My account login isn't working after the update\" helps identify key terms (\"account\", \"login\", \"working\", \"update\") that can be tracked across thousands of support tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: My account login isn't working after the update\n",
      "Tokens: ['My', 'account', 'login', \"isn't\", 'working', 'after', 'the', 'update']\n",
      "\n",
      "Email: I can't log into my account since the latest update\n",
      "Tokens: ['I', \"can't\", 'log', 'into', 'my', 'account', 'since', 'the', 'latest', 'update']\n",
      "\n",
      "Email: The app keeps crashing when I try to reset my password\n",
      "Tokens: ['The', 'app', 'keeps', 'crashing', 'when', 'I', 'try', 'to', 'reset', 'my', 'password']\n",
      "\n",
      "Email: After updating, the login screen freezes when I enter my passwordThe app KEEPS CRASHING\n",
      "Tokens: ['After', 'updating,', 'the', 'login', 'screen', 'freezes', 'when', 'I', 'enter', 'my', 'passwordThe', 'app', 'KEEPS', 'CRASHING']\n",
      "\n",
      "Token frequencies:\n",
      "the: 3\n",
      "I: 3\n",
      "my: 3\n",
      "account: 2\n",
      "login: 2\n",
      "update: 2\n",
      "app: 2\n",
      "when: 2\n",
      "My: 1\n",
      "isn't: 1\n",
      "working: 1\n",
      "after: 1\n",
      "can't: 1\n",
      "log: 1\n",
      "into: 1\n",
      "since: 1\n",
      "latest: 1\n",
      "The: 1\n",
      "keeps: 1\n",
      "crashing: 1\n",
      "try: 1\n",
      "to: 1\n",
      "reset: 1\n",
      "password: 1\n",
      "After: 1\n",
      "updating,: 1\n",
      "screen: 1\n",
      "freezes: 1\n",
      "enter: 1\n",
      "passwordThe: 1\n",
      "KEEPS: 1\n",
      "CRASHING: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sample customer support emails\n",
    "sample_emails = [\n",
    "    \"My account login isn't working after the update\",\n",
    "    \"I can't log into my account since the latest update\",\n",
    "    \"The app keeps crashing when I try to reset my password\",\n",
    "    \"After updating, the login screen freezes when I enter my password\"\n",
    "    \"The app KEEPS CRASHING\"\n",
    "]\n",
    "\n",
    "# Function to tokenize an email into words\n",
    "def tokenize_email(email):    \n",
    "    \n",
    "    # Split into words\n",
    "    tokens = email.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Process all emails\n",
    "all_tokens = []\n",
    "\n",
    "# Tokenize each email and add tokens to our collection\n",
    "for email in sample_emails:\n",
    "    tokens = tokenize_email(email)\n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Count token frequencies using Counter\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Print the most common tokens\n",
    "print(\"Token frequencies:\")\n",
    "for token, count in token_counts.most_common():\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Normalization**: Converting text to a standard form to reduce variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "print(normalized_tokens)\n",
    "# Output: ['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']\n",
    "\n",
    "# Removing punctuation\n",
    "import re\n",
    "normalized_tokens = [re.sub(r'[^\\w\\s]', '', token.lower()) for token in tokens]\n",
    "print(normalized_tokens)\n",
    "# Output: ['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fix the previous example by combining words like my and My, crashing and CRASHING, the and The...etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: My account login isn't working after the update\n",
      "Tokens: ['my', 'account', 'login', 'isn', 't', 'working', 'after', 'the', 'update']\n",
      "\n",
      "Email: I can't log into my account since the latest update\n",
      "Tokens: ['i', 'can', 't', 'log', 'into', 'my', 'account', 'since', 'the', 'latest', 'update']\n",
      "\n",
      "Email: The app keeps crashing when I try to reset my password\n",
      "Tokens: ['the', 'app', 'keeps', 'crashing', 'when', 'i', 'try', 'to', 'reset', 'my', 'password']\n",
      "\n",
      "Email: After updating, the login screen freezes when I enter my passwordThe app KEEPS CRASHING\n",
      "Tokens: ['after', 'updating', 'the', 'login', 'screen', 'freezes', 'when', 'i', 'enter', 'my', 'passwordthe', 'app', 'keeps', 'crashing']\n",
      "\n",
      "Token frequencies:\n",
      "my: 4\n",
      "the: 4\n",
      "i: 3\n",
      "account: 2\n",
      "login: 2\n",
      "t: 2\n",
      "after: 2\n",
      "update: 2\n",
      "app: 2\n",
      "keeps: 2\n",
      "crashing: 2\n",
      "when: 2\n",
      "isn: 1\n",
      "working: 1\n",
      "can: 1\n",
      "log: 1\n",
      "into: 1\n",
      "since: 1\n",
      "latest: 1\n",
      "try: 1\n",
      "to: 1\n",
      "reset: 1\n",
      "password: 1\n",
      "updating: 1\n",
      "screen: 1\n",
      "freezes: 1\n",
      "enter: 1\n",
      "passwordthe: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sample customer support emails\n",
    "sample_emails = [\n",
    "    \"My account login isn't working after the update\",\n",
    "    \"I can't log into my account since the latest update\",\n",
    "    \"The app keeps crashing when I try to reset my password\",\n",
    "    \"After updating, the login screen freezes when I enter my password\"\n",
    "    \"The app KEEPS CRASHING\"\n",
    "]\n",
    "\n",
    "# Function to tokenize an email into words\n",
    "def tokenize_email(email):\n",
    "    # Convert to lowercase\n",
    "    email = email.lower()\n",
    "    \n",
    "    # Remove special characters and replace with space\n",
    "    email = re.sub(r'[^a-zA-Z0-9\\s]', ' ', email)\n",
    "    \n",
    "    # Split into words\n",
    "    tokens = email.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Process all emails\n",
    "all_tokens = []\n",
    "\n",
    "# Tokenize each email and add tokens to our collection\n",
    "for email in sample_emails:\n",
    "    tokens = tokenize_email(email)\n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Count token frequencies using Counter\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Print the most common tokens\n",
    "print(\"Token frequencies:\")\n",
    "for token, count in token_counts.most_common():\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Stopword Removal**: Eliminating common words that add little meaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify the previous example to remove stopwords so that we can focus on important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: My account login isn't working after the update\n",
      "Tokens after stopword removal: ['account', 'login', 'working', 'update']\n",
      "\n",
      "Email: I can't log into my account since the latest update\n",
      "Tokens after stopword removal: ['log', 'account', 'since', 'latest', 'update']\n",
      "\n",
      "Email: The app keeps crashing when I try to reset my password\n",
      "Tokens after stopword removal: ['app', 'keeps', 'crashing', 'try', 'reset', 'password']\n",
      "\n",
      "Email: After updating, the login screen freezes when I enter my passwordThe app KEEPS CRASHING\n",
      "Tokens after stopword removal: ['updating', 'login', 'screen', 'freezes', 'enter', 'passwordthe', 'app', 'keeps', 'crashing']\n",
      "\n",
      "Token frequencies:\n",
      "account: 2\n",
      "login: 2\n",
      "update: 2\n",
      "app: 2\n",
      "keeps: 2\n",
      "crashing: 2\n",
      "working: 1\n",
      "log: 1\n",
      "since: 1\n",
      "latest: 1\n",
      "try: 1\n",
      "reset: 1\n",
      "password: 1\n",
      "updating: 1\n",
      "screen: 1\n",
      "freezes: 1\n",
      "enter: 1\n",
      "passwordthe: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sample customer support emails\n",
    "sample_emails = [\n",
    "    \"My account login isn't working after the update\",\n",
    "    \"I can't log into my account since the latest update\",\n",
    "    \"The app keeps crashing when I try to reset my password\",\n",
    "    \"After updating, the login screen freezes when I enter my password\"\n",
    "    \"The app KEEPS CRASHING\"\n",
    "]\n",
    "\n",
    "# Define a list of common English stopwords\n",
    "stopwords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "    'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "    'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "    'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "    'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "    'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', \n",
    "    'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "    'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "    's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'isn', 'aren'\n",
    "]\n",
    "\n",
    "# Function to tokenize an email into words with stopword removal\n",
    "def tokenize_email(email):\n",
    "    # Convert to lowercase\n",
    "    email = email.lower()\n",
    "    \n",
    "    # Remove special characters and replace with space\n",
    "    email = re.sub(r'[^a-zA-Z0-9\\s]', ' ', email)\n",
    "    \n",
    "    # Split into words\n",
    "    tokens = email.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Process all emails\n",
    "all_tokens = []\n",
    "\n",
    "# Tokenize each email and add tokens to our collection\n",
    "for email in sample_emails:\n",
    "    tokens = tokenize_email(email)\n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"Tokens after stopword removal: {tokens}\\n\")\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Count token frequencies using Counter\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Print the most common tokens\n",
    "print(\"Token frequencies:\")\n",
    "for token, count in token_counts.most_common():\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use NLTK to remove stopwords using its built-in stopwords dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: My account login isn't working after the update\n",
      "Tokens after NLTK processing: ['account', 'login', 'working', 'update']\n",
      "\n",
      "Email: I can't log into my account since the latest update\n",
      "Tokens after NLTK processing: ['ca', 'log', 'account', 'since', 'latest', 'update']\n",
      "\n",
      "Email: The app keeps crashing when I try to reset my password\n",
      "Tokens after NLTK processing: ['app', 'keeps', 'crashing', 'try', 'reset', 'password']\n",
      "\n",
      "Email: After updating, the login screen freezes when I enter my passwordThe app KEEPS CRASHING\n",
      "Tokens after NLTK processing: ['updating', 'login', 'screen', 'freezes', 'enter', 'passwordthe', 'app', 'keeps', 'crashing']\n",
      "\n",
      "Token frequencies:\n",
      "account: 2\n",
      "login: 2\n",
      "update: 2\n",
      "app: 2\n",
      "keeps: 2\n",
      "crashing: 2\n",
      "working: 1\n",
      "ca: 1\n",
      "log: 1\n",
      "since: 1\n",
      "latest: 1\n",
      "try: 1\n",
      "reset: 1\n",
      "password: 1\n",
      "updating: 1\n",
      "screen: 1\n",
      "freezes: 1\n",
      "enter: 1\n",
      "passwordthe: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/me/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/me/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample customer support emails\n",
    "sample_emails = [\n",
    "    \"My account login isn't working after the update\",\n",
    "    \"I can't log into my account since the latest update\",\n",
    "    \"The app keeps crashing when I try to reset my password\",\n",
    "    \"After updating, the login screen freezes when I enter my password\"\n",
    "    \"The app KEEPS CRASHING\"\n",
    "]\n",
    "\n",
    "# Function to tokenize an email and remove stopwords using NLTK\n",
    "def tokenize_email(email):\n",
    "    # Convert to lowercase\n",
    "    email = email.lower()\n",
    "    \n",
    "    # Tokenize using NLTK\n",
    "    tokens = word_tokenize(email)\n",
    "    \n",
    "    # Remove stopwords using NLTK's built-in stopwords list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Process all emails\n",
    "all_tokens = []\n",
    "\n",
    "# Tokenize each email and add tokens to our collection\n",
    "for email in sample_emails:\n",
    "    tokens = tokenize_email(email)\n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"Tokens after NLTK processing: {tokens}\\n\")\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Count token frequencies using Counter\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Print the most common tokens\n",
    "print(\"Token frequencies:\")\n",
    "for token, count in token_counts.most_common():\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 tokens in customer emails:\n",
      "updat: 6\n",
      "account: 4\n",
      "password: 4\n",
      "login: 3\n",
      "log: 3\n",
      "reset: 3\n",
      "work: 2\n",
      "app: 2\n",
      "crash: 2\n",
      "tri: 2\n",
      "\n",
      "Email count: 10\n",
      "'login' and 'update' appear together in 3 emails (30.0%)\n",
      "'password' and 'reset' appear together in 3 emails (30.0%)\n",
      "\n",
      "Token frequency visualization saved as 'token_frequency.png'\n",
      "\n",
      "Issue counts by category:\n",
      "login_issues: 7 (70.0%)\n",
      "password_issues: 4 (40.0%)\n",
      "app_performance: 3 (30.0%)\n",
      "display_issues: 1 (10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/me/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download required NLTK data (only needed once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample customer support emails\n",
    "sample_emails = [\n",
    "    \"My account login isn't working after the update\",\n",
    "    \"I can't log into my account since the latest update\",\n",
    "    \"The app keeps crashing when I try to reset my password\",\n",
    "    \"After updating, the login screen freezes when I enter my password\",\n",
    "    \"My account balance is not showing correctly after logging in\",\n",
    "    \"The password reset link in my email doesn't work\",\n",
    "    \"I'm having trouble logging in with the new update\",\n",
    "    \"The app crashes every time I try to access account settings\",\n",
    "    \"Can't verify my identity when attempting to login after update\",\n",
    "    \"The new update is great but I can't reset my password now\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'email_text': sample_emails})\n",
    "\n",
    "# Function to tokenize and clean text\n",
    "def tokenize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and replace with space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize (split into words)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming (reducing words to their root form)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to emails\n",
    "df['tokens'] = df['email_text'].apply(tokenize_text)\n",
    "\n",
    "# Count all tokens\n",
    "all_tokens = []\n",
    "for token_list in df['tokens']:\n",
    "    all_tokens.extend(token_list)\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "print(\"Top 10 tokens in customer emails:\")\n",
    "for token, count in token_counts.most_common(10):\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "# Find co-occurrences of specific terms\n",
    "def find_cooccurrence(token1, token2, token_lists):\n",
    "    count = 0\n",
    "    for tokens in token_lists:\n",
    "        if token1 in tokens and token2 in tokens:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Check some meaningful co-occurrences\n",
    "login_update = find_cooccurrence('login', 'updat', df['tokens'])\n",
    "password_reset = find_cooccurrence('password', 'reset', df['tokens'])\n",
    "\n",
    "print(f\"\\nEmail count: {len(df)}\")\n",
    "print(f\"'login' and 'update' appear together in {login_update} emails ({login_update/len(df)*100:.1f}%)\")\n",
    "print(f\"'password' and 'reset' appear together in {password_reset} emails ({password_reset/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize the top tokens\n",
    "plt.figure(figsize=(10, 6))\n",
    "common_tokens = dict(token_counts.most_common(8))\n",
    "plt.bar(common_tokens.keys(), common_tokens.values())\n",
    "plt.title('Most Common Terms in Customer Support Emails')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('token_frequency.png')\n",
    "print(\"\\nToken frequency visualization saved as 'token_frequency.png'\")\n",
    "\n",
    "# Basic issue categorization\n",
    "def categorize_issue(tokens):\n",
    "    categories = {\n",
    "        'login_issues': ['login', 'log', 'account'],\n",
    "        'password_issues': ['password', 'reset'],\n",
    "        'app_performance': ['crash', 'freez', 'slow'],\n",
    "        'display_issues': ['show', 'display', 'balanc']\n",
    "    }\n",
    "    \n",
    "    result = {}\n",
    "    for category, category_tokens in categories.items():\n",
    "        if any(token in tokens for token in category_tokens):\n",
    "            result[category] = True\n",
    "        else:\n",
    "            result[category] = False\n",
    "    return result\n",
    "\n",
    "# Apply categorization\n",
    "categories_df = pd.DataFrame(df['tokens'].apply(categorize_issue).tolist())\n",
    "df = pd.concat([df, categories_df], axis=1)\n",
    "\n",
    "# Count issues by category\n",
    "category_counts = categories_df.sum()\n",
    "print(\"\\nIssue counts by category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count} ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More NLP Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Part-of-speech (POS) tagging**: Identifying whether words are nouns, verbs, adjectives, etc.\n",
    "   - \"The quick brown fox jumps\" → [Determiner, Adjective, Adjective, Noun, Verb]\n",
    "   - Critical for understanding grammatical structure and word meaning in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/POS-tagging.jpg\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penn Treebank POS Tags (commonly used in NLTK and other NLP tools)\n",
    "\n",
    "a. **Nouns**\n",
    "   - NN: Noun, singular (book, cat)\n",
    "   - NNS: Noun, plural (books, cats)\n",
    "   - NNP: Proper noun, singular (John, London)\n",
    "   - NNPS: Proper noun, plural (Americans, Alps)\n",
    "\n",
    "b. **Verbs**\n",
    "   - VB: Verb, base form (take)\n",
    "   - VBD: Verb, past tense (took)\n",
    "   - VBG: Verb, gerund/present participle (taking)\n",
    "   - VBN: Verb, past participle (taken)\n",
    "   - VBP: Verb, non-3rd person singular present (take)\n",
    "   - VBZ: Verb, 3rd person singular present (takes)\n",
    "\n",
    "c. **Adjectives**\n",
    "   - JJ: Adjective (big)\n",
    "   - JJR: Adjective, comparative (bigger)\n",
    "   - JJS: Adjective, superlative (biggest)\n",
    "\n",
    "d. **Adverbs**\n",
    "   - RB: Adverb (quickly, not)\n",
    "   - RBR: Adverb, comparative (faster)\n",
    "   - RBS: Adverb, superlative (fastest)\n",
    "\n",
    "e. **Pronouns**\n",
    "   - PRP: Personal pronoun (I, you, he)\n",
    "   - PRP$: Possessive pronoun (my, your)\n",
    "\n",
    "f. **Determiners**\n",
    "   - DT: Determiner (the, a, these)\n",
    "   - WDT: Wh-determiner (which, that)\n",
    "\n",
    "g. **Prepositions**\n",
    "   - IN: Preposition or subordinating conjunction (in, of, like)\n",
    "\n",
    "h. **Conjunctions**\n",
    "   - CC: Coordinating conjunction (and, but, or)\n",
    "\n",
    "h. **Particles**\n",
    "   - RP: Particle (up, off)\n",
    "\n",
    "i. **Other Common Tags**\n",
    "    - CD: Cardinal number (one, two)\n",
    "    - MD: Modal (can, should)\n",
    "    - TO: to\n",
    "    - WRB: Wh-adverb (where, when)\n",
    "    - UH: Interjection (oh, wow)\n",
    "    - FW: Foreign word\n",
    "    - SYM: Symbol\n",
    "    - .: Period\n",
    "    - ,: Comma\n",
    "    - :: Colon\n",
    "\n",
    "These tags are used in most POS tagging systems, though some variations exist between different NLP libraries and annotation schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application:** E-commerce companies use POS tagging to automatically extract product features (nouns) and associated sentiments (adjectives) from thousands of reviews, helping product teams prioritize improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/me/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/me/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # POS tagger \n",
    "\n",
    "\n",
    "# Sample product review\n",
    "review = \"This smartphone has an amazing camera but the battery life is terrible.\"\n",
    "\n",
    "# Tokenize and perform POS tagging\n",
    "tokens = word_tokenize(review)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "print(\"POS Tagged Review:\")\n",
    "print(tagged)\n",
    "\n",
    "# Extract adjectives which often indicate product qualities\n",
    "adjectives = []\n",
    "for word, tag in tagged:\n",
    "    if tag.startswith('JJ'):\n",
    "        adjectives.append(word)\n",
    "print(f\"\\nProduct qualities (adjectives): {adjectives}\")\n",
    "\n",
    "# Extract nouns which often indicate product features\n",
    "nouns = []\n",
    "for word, tag in tagged:\n",
    "    if tag.startswith('NN'):\n",
    "        nouns.append(word)\n",
    "print(f\"Product features (nouns): {nouns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Parsing**: Analyzing grammatical structure of sentences\n",
    "   - Constituency parsing: Constructing phrase structure trees\n",
    "   - Dependency parsing: Identifying grammatical relationships between words\n",
    "   - Semantic parsing: Mapping sentences to formal meaning representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/parsing.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application:** Grammar checking software like Grammarly uses syntactic parsing to identify and correct grammatical errors in real-time, helping millions of users improve their writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we donwload the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/me/myenv/lib/python3.12/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/me/myenv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/me/myenv/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/me/myenv/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/me/myenv/lib/python3.12/site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /home/me/myenv/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/me/myenv/lib/python3.12/site-packages (from spacy) (75.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/me/myenv/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/me/myenv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/me/myenv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/me/myenv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/me/myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/me/myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/me/myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/me/myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/me/myenv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/me/myenv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/me/myenv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/me/myenv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/me/myenv/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/me/myenv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/me/myenv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/me/myenv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/me/myenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.6/138.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n",
      "/home/me/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing incorrect sentence:\n",
      "Subject: 'students' (NNS) | Verb: 'finished' (VBN)\n",
      "AGREEMENT ERROR: Plural subject with singular verb\n",
      "\n",
      "Analyzing correct sentence:\n",
      "Subject: 'students' (NNS) | Verb: 'finished' (VBN)\n",
      "AGREEMENT ERROR: Plural subject with singular verb\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample sentence with subject-verb agreement error\n",
    "incorrect = \"The students in my class has finished their exams.\"\n",
    "correct = \"The students in my class have finished their exams.\"\n",
    "\n",
    "# Parse both sentences\n",
    "doc_incorrect = nlp(incorrect)\n",
    "doc_correct = nlp(correct)\n",
    "\n",
    "# Function to check subject-verb agreement\n",
    "def check_subject_verb_agreement(doc):\n",
    "    for token in doc:\n",
    "        # Find main verbs\n",
    "        if token.pos_ == \"VERB\" and token.dep_ == \"ROOT\":\n",
    "            # Find the subject\n",
    "            subjects = [subj for subj in token.children if subj.dep_ == \"nsubj\"]\n",
    "            if subjects:\n",
    "                subject = subjects[0]\n",
    "                print(f\"Subject: '{subject.text}' ({subject.tag_}) | Verb: '{token.text}' ({token.tag_})\")\n",
    "                # Check if plural subject has plural verb form\n",
    "                if subject.tag_ == \"NNS\" and token.tag_ not in [\"VBP\", \"VB\"]:\n",
    "                    print(f\"AGREEMENT ERROR: Plural subject with singular verb\")\n",
    "                elif subject.tag_ == \"NN\" and token.tag_ == \"VBP\":\n",
    "                    print(f\"AGREEMENT ERROR: Singular subject with plural verb\")\n",
    "                else:\n",
    "                    print(\"Subject-verb agreement is correct\")\n",
    "\n",
    "print(\"Analyzing incorrect sentence:\")\n",
    "check_subject_verb_agreement(doc_incorrect)\n",
    "\n",
    "print(\"\\nAnalyzing correct sentence:\")\n",
    "check_subject_verb_agreement(doc_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Stemming vs. Lemmatization in NLP**\n",
    "\n",
    "Stemming and lemmatization are text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base or root forms. They help in improving search engines, text mining, and language modeling by standardizing words with similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/stemming_lemmatization.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. Stemming**\n",
    "**Definition**: Stemming is a rule-based process that removes prefixes and suffixes (affixes) from a word to obtain its base form, often producing words that may not be actual words.\n",
    "\n",
    "- **Method**: Uses algorithms like **Porter Stemmer** and **Snowball Stemmer**.\n",
    "- **Example**:\n",
    "  - \"Running\" → \"Run\"\n",
    "  - \"Studies\" → \"Studi\"\n",
    "  - \"Better\" → \"Better\" (Incorrectly unchanged)\n",
    "  - \"Caring\" → \"Car\"\n",
    "  \n",
    "- **Limitations**:\n",
    "  - Often results in words that are not real words (\"Studies\" → \"Studi\").\n",
    "  - Does not consider the actual meaning of words.\n",
    "\n",
    "**ii. Lemmatization**\n",
    "**Definition**: Lemmatization is a dictionary-based approach that reduces words to their meaningful root form (lemma), considering the word's context and part of speech.\n",
    "\n",
    "- **Method**: Uses linguistic analysis and dictionaries (like WordNet Lemmatizer).\n",
    "- **Example**:\n",
    "  - \"Running\" → \"Run\"\n",
    "  - \"Studies\" → \"Study\"\n",
    "  - \"Better\" → \"Good\" (Correct Lemma)\n",
    "  - \"Caring\" → \"Care\"\n",
    "  \n",
    "- **Advantages**:\n",
    "  - More accurate than stemming.\n",
    "  - Produces real words with proper meanings.\n",
    "  \n",
    "**Key Differences**\n",
    "| Feature | Stemming | Lemmatization |\n",
    "|---------|---------|--------------|\n",
    "| **Approach** | Rule-based | Dictionary-based |\n",
    "| **Speed** | Faster | Slower |\n",
    "| **Accuracy** | Less accurate | More accurate |\n",
    "| **Example** | \"Caring\" → \"Car\" | \"Caring\" → \"Care\" |\n",
    "\n",
    "**Use Case**:  \n",
    "- **Stemming** is useful when **speed** is more important than accuracy (e.g., search engines).  \n",
    "- **Lemmatization** is better for **semantic analysis** where meaning matters (e.g., AI chatbots, NLP applications)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application:** Search engines use stemming and lemmatization to match user queries with relevant content even when the exact words don't match. SEO tools help content creators optimize for variants of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/me/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running shoes for marathon\n",
      "Stemmed: run shoe for marathon\n",
      "Lemmatized: running shoe for marathon\n",
      "\n",
      "Original: runner shoes for marathons\n",
      "Stemmed: runner shoe for marathon\n",
      "Lemmatized: runner shoe for marathon\n",
      "\n",
      "Original: shoes for marathon runners\n",
      "Stemmed: shoe for marathon runner\n",
      "Lemmatized: shoe for marathon runner\n",
      "\n",
      "Original: best shoes to run marathons\n",
      "Stemmed: best shoe to run marathon\n",
      "Lemmatized: best shoe to run marathon\n",
      "\n",
      "Query groups based on stemmed keywords:\n",
      "Group: for marathon run shoe\n",
      "Queries: ['running shoes for marathon']\n",
      "\n",
      "Group: for marathon runner shoe\n",
      "Queries: ['runner shoes for marathons', 'shoes for marathon runners']\n",
      "\n",
      "Group: best marathon run shoe to\n",
      "Queries: ['best shoes to run marathons']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set of related query variants\n",
    "search_queries = [\n",
    "    \"running shoes for marathon\",\n",
    "    \"runner shoes for marathons\",\n",
    "    \"shoes for marathon runners\",\n",
    "    \"best shoes to run marathons\"\n",
    "]\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Process each query\n",
    "for query in search_queries:\n",
    "    tokens = query.lower().split()\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    print(f\"Original: {query}\")\n",
    "    print(f\"Stemmed: {' '.join(stemmed)}\")\n",
    "    print(f\"Lemmatized: {' '.join(lemmatized)}\\n\")\n",
    "\n",
    "# Keyword grouping based on stems\n",
    "stem_to_queries = {}\n",
    "for query in search_queries:\n",
    "    # Create a query signature using stems\n",
    "    tokens = query.lower().split()\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    signature = \" \".join(sorted(set(stemmed)))\n",
    "    \n",
    "    if signature not in stem_to_queries:\n",
    "        stem_to_queries[signature] = []\n",
    "    stem_to_queries[signature].append(query)\n",
    "\n",
    "print(\"Query groups based on stemmed keywords:\")\n",
    "for signature, queries in stem_to_queries.items():\n",
    "    print(f\"Group: {signature}\")\n",
    "    print(f\"Queries: {queries}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**6. Named Entity Recognition (NER)**: Identifying proper nouns like people, organizations, locations\n",
    "   - \"Apple is releasing a new iPhone in San Francisco\" → [Organization, Product, Location]\n",
    "   - Useful for extracting key information from unstructured text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/ner.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application:** Media monitoring companies use NER to track mentions of clients, competitors, and industry trends across thousands of news sources, enabling real-time alerts and competitive intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Named Entities:\n",
      "Organizations: Apple Inc.\n",
      "Persons: Tim Cook\n",
      "Locations: Austin, Texas\n",
      "Dates: 2025, yesterday\n",
      "Money: $1 billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample news article snippet\n",
    "news_text = \"Apple Inc. is planning to open a new campus in Austin, Texas by 2025. CEO Tim Cook announced the $1 billion investment yesterday.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(news_text)\n",
    "\n",
    "# Extract and categorize named entities\n",
    "entities = {\n",
    "    \"Organizations\": [],\n",
    "    \"Persons\": [],\n",
    "    \"Locations\": [],\n",
    "    \"Dates\": [],\n",
    "    \"Money\": []\n",
    "}\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":\n",
    "        entities[\"Organizations\"].append(ent.text)\n",
    "    elif ent.label_ == \"PERSON\":\n",
    "        entities[\"Persons\"].append(ent.text)\n",
    "    elif ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "        entities[\"Locations\"].append(ent.text)\n",
    "    elif ent.label_ == \"DATE\":\n",
    "        entities[\"Dates\"].append(ent.text)\n",
    "    elif ent.label_ == \"MONEY\":\n",
    "        entities[\"Money\"].append(ent.text)\n",
    "\n",
    "print(\"Extracted Named Entities:\")\n",
    "for category, items in entities.items():\n",
    "    print(f\"{category}: {', '.join(items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Coreference resolution**: Determining when different words refer to the same entity\n",
    "   - \"John said he was tired\" → \"he\" refers to \"John\"\n",
    "   - Essential for understanding relationships across sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-LLMs/main/lecture%20notes/images/coreference.png\" alt=\"Text Tokenization\" width=\"600\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world application:** Automated customer service systems use coreference resolution to track what \"it\" refers to throughout a conversation, enabling more natural interactions and accurate information extraction for ticketing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Text: Bag of Words and TF-IDF\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "A simple way to represent text as numerical vectors by counting word occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'applications' 'benefit' 'business' 'data' 'from' 'language' 'large'\n",
      " 'learn' 'models' 'revolutionize' 'text']\n",
      "[[0 0 0 1 0 0 1 1 0 1 1 0]\n",
      " [1 1 1 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Large language models revolutionize business.\",\n",
    "    \"Business applications benefit from AI.\",\n",
    "    \"Language models learn from text data.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# Output: ['ai', 'applications', 'benefit', 'business', 'data', 'from', 'language', 'large', 'learn', 'models', 'revolutionize', 'text']\n",
    "\n",
    "print(X.toarray())\n",
    "# Output: \n",
    "# [[0 0 0 1 0 0 1 1 0 1 1 0]\n",
    "#  [1 1 1 1 0 1 0 0 0 0 0 0]\n",
    "#  [0 0 0 0 1 1 1 0 1 1 0 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world Application:** A human resources department can use Bag of Words to automatically categorize job applications. By converting resumes to BoW vectors, the system can identify which candidates mention key skills like \"Python\", \"machine learning\", or \"project management\", enabling faster screening of large applicant pools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "A more sophisticated approach that weights terms based on their importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'applications' 'benefit' 'business' 'data' 'from' 'language' 'large'\n",
      " 'learn' 'models' 'revolutionize' 'text']\n",
      "[[0.         0.         0.         0.3935112  0.         0.\n",
      "  0.3935112  0.51741994 0.         0.3935112  0.51741994 0.        ]\n",
      " [0.49047908 0.49047908 0.49047908 0.37302199 0.         0.37302199\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.45954803 0.34949812\n",
      "  0.34949812 0.         0.45954803 0.34949812 0.         0.45954803]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "# Same output as above\n",
    "\n",
    "print(X_tfidf.toarray())\n",
    "# Output will be a matrix of TF-IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-world Application:** News recommendation systems use TF-IDF to identify distinctive topics in articles. When a reader shows interest in technology articles, the system can recommend other articles with similar TF-IDF profiles, focusing on meaningful terms like \"blockchain\" or \"artificial intelligence\" rather than common words like \"said\" or \"reported\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Language Models: N-grams\n",
    "\n",
    "N-gram models were among the earliest statistical language models, predicting the probability of a sequence of words by analyzing patterns in training data.\n",
    "\n",
    "### What are N-grams?\n",
    "\n",
    "N-grams are contiguous sequences of n items (words, characters, etc.) from a text:\n",
    "- **Unigrams**: Single words (e.g., \"language\")\n",
    "- **Bigrams**: Two consecutive words (e.g., \"language models\")\n",
    "- **Trigrams**: Three consecutive words (e.g., \"large language models\")\n",
    "\n",
    "### Implementing N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('large', 'language'), ('language', 'models'), ('models', 'are'), ('are', 'transforming'), ('transforming', 'how'), ('how', 'businesses'), ('businesses', 'operate')]\n",
      "[('large', 'language', 'models'), ('language', 'models', 'are'), ('models', 'are', 'transforming'), ('are', 'transforming', 'how'), ('transforming', 'how', 'businesses'), ('how', 'businesses', 'operate')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"Large language models are transforming how businesses operate\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams_list = list(ngrams(tokens, 2))\n",
    "print(bigrams_list)\n",
    "# Output: [('large', 'language'), ('language', 'models'), ('models', 'are'), ('are', 'transforming'), ('transforming', 'how'), ('how', 'businesses'), ('businesses', 'operate')]\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams_list = list(ngrams(tokens, 3))\n",
    "print(trigrams_list)\n",
    "# Output: [('large', 'language', 'models'), ('language', 'models', 'are'), ('models', 'are', 'transforming'), ('are', 'transforming', 'how'), ('transforming', 'how', 'businesses'), ('how', 'businesses', 'operate')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple N-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language models for customer service , content creation , and data analysis . language models for customer service , content creation ,\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_ngram_model(text, n=2):\n",
    "    \"\"\"Build an n-gram language model from text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    ngrams_dict = defaultdict(list)\n",
    "    \n",
    "    # Create dictionary of n-grams and possible next words\n",
    "    for i in range(len(tokens) - n):\n",
    "        current_ngram = tuple(tokens[i:i+n])\n",
    "        next_word = tokens[i+n]\n",
    "        ngrams_dict[current_ngram].append(next_word)\n",
    "    \n",
    "    return ngrams_dict\n",
    "\n",
    "def generate_text(model, seed, length=20):\n",
    "    \"\"\"Generate text using the n-gram model.\"\"\"\n",
    "    current = seed\n",
    "    result = list(seed)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current in model:\n",
    "            # Randomly select a possible next word\n",
    "            next_word = random.choice(model[current])\n",
    "            result.append(next_word)\n",
    "            # Update current n-gram\n",
    "            current = current[1:] + (next_word,)\n",
    "        else:\n",
    "            # If current n-gram is not in model, break\n",
    "            break\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = \"\"\"Large language models are transforming how businesses operate. \n",
    "These models can understand language, generate text, and perform various tasks. \n",
    "Businesses use language models for customer service, content creation, and data analysis.\n",
    "Language models learn patterns from vast amounts of text data.\"\"\"\n",
    "\n",
    "# Build a bigram model\n",
    "bigram_model = build_ngram_model(corpus, 2)\n",
    "\n",
    "# Generate text using the model\n",
    "seed = ('language', 'models')\n",
    "generated_text = generate_text(bigram_model, seed)\n",
    "print(generated_text)\n",
    "# Output might be: \"language models learn patterns from vast amounts of text data businesses use language models for customer service content creation and data analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of N-gram Models\n",
    "\n",
    "1. **Limited context**: Only consider a fixed number of previous words\n",
    "2. **Data sparsity**: Many valid word combinations don't appear in training data\n",
    "3. **No semantic understanding**: Model captures statistical patterns but not meaning\n",
    "4. **Memory intensive**: Storing all possible n-grams requires significant space\n",
    "5. **No long-range dependencies**: Cannot capture relationships between distant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Techniques\n",
    "\n",
    "To address data sparsity in n-gram models, smoothing techniques redistribute probability mass to unseen events:\n",
    "\n",
    "### Laplace (Add-One) Smoothin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(models|language) = 0.1081\n"
     ]
    }
   ],
   "source": [
    "def build_bigram_model_with_smoothing(text):\n",
    "    \"\"\"Build a bigram model with Laplace smoothing.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Count word frequencies\n",
    "    unigram_counts = defaultdict(int)\n",
    "    bigram_counts = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        unigram_counts[tokens[i]] += 1\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "    \n",
    "    # Calculate probabilities with smoothing\n",
    "    vocab_size = len(unigram_counts)\n",
    "    bigram_model = {}\n",
    "    \n",
    "    for bigram, count in bigram_counts.items():\n",
    "        # P(w2|w1) = (count(w1,w2) + 1) / (count(w1) + V)\n",
    "        w1 = bigram[0]\n",
    "        bigram_model[bigram] = (count + 1) / (unigram_counts[w1] + vocab_size)\n",
    "    \n",
    "    return bigram_model, unigram_counts, vocab_size\n",
    "\n",
    "# Build model with smoothing\n",
    "model, unigram_counts, vocab_size = build_bigram_model_with_smoothing(corpus)\n",
    "\n",
    "# Calculate probability of a bigram\n",
    "def get_bigram_probability(model, unigram_counts, vocab_size, w1, w2):\n",
    "    \"\"\"Get the probability of a bigram with smoothing.\"\"\"\n",
    "    if (w1, w2) in model:\n",
    "        return model[(w1, w2)]\n",
    "    else:\n",
    "        # For unseen bigrams\n",
    "        return 1 / (unigram_counts[w1] + vocab_size)\n",
    "\n",
    "# Example probability\n",
    "prob = get_bigram_probability(model, unigram_counts, vocab_size, 'language', 'models')\n",
    "print(f\"P(models|language) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Language Models: Perplexity\n",
    "Perplexity measures how well a language model predicts a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 28.45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_perplexity(test_text, model, unigram_counts, vocab_size):\n",
    "    \"\"\"Calculate perplexity of test text using the bigram model.\"\"\"\n",
    "    tokens = word_tokenize(test_text.lower())\n",
    "    log_probability = 0\n",
    "    \n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i+1])\n",
    "        probability = get_bigram_probability(model, unigram_counts, vocab_size, tokens[i], tokens[i+1])\n",
    "        log_probability += np.log2(probability)\n",
    "    \n",
    "    # Perplexity = 2^(-average log probability)\n",
    "    perplexity = 2 ** (-log_probability / (len(tokens) - 1))\n",
    "    return perplexity\n",
    "\n",
    "# Test the model on new text\n",
    "test_text = \"Language models help businesses understand customer feedback.\"\n",
    "perplexity = calculate_perplexity(test_text, model, unigram_counts, vocab_size)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lecture, we've explored the foundations of NLP and classical language models, particularly focusing on n-gram models. While these approaches have significant limitations, they introduced important concepts that laid the groundwork for more advanced language models.\n",
    "\n",
    "In the next lecture, we'll examine the transition from statistical models to neural network-based approaches, including word embeddings and recurrent neural networks, which addressed many of the limitations of classical models.\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. How might a business use basic NLP techniques like tokenization and sentiment analysis to gain insights from customer feedback?\n",
    "2. What are the key limitations of the bag-of-words approach for representing text?\n",
    "3. How could n-gram models be used in predictive text applications, and what challenges might arise?\n",
    "4. Why is context so important in language understanding, and how do classical models struggle with this?\n",
    "\n",
    "## Practical Assignment\n",
    "\n",
    "Implement a simple text classifier using the bag-of-words model and TF-IDF to categorize customer feedback as positive, negative, or neutral. Compare the performance of these two text representation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## غير مطلوب - مطالعة ذاتية\n",
    "## Additional Language Challenges\n",
    "\n",
    "- **Context-dependent**: Meaning often depends on surrounding text or real-world knowledge\n",
    "  - **Lexical context**: The meaning of words depends on surrounding words\n",
    "    - Example: In \"She took the bat to the game\" vs. \"The bat flew into the cave,\" the meaning of \"bat\" changes completely\n",
    "    - Example: \"He was drawing a bow\" (musical instrument or weapon?) depends on previous sentences\n",
    "    - Example: \"She went to the bank\" (financial institution or riverbank?) requires surrounding context\n",
    "    - Computational challenge: Models must use nearby words to disambiguate meanings\n",
    "  \n",
    "  - **Discourse context**: The meaning depends on previous statements or conversation history\n",
    "    - Example: \"She likes it\" is meaningless without knowing what \"it\" refers to\n",
    "    - Example: \"That's not what I meant\" requires understanding previous utterances\n",
    "    - Example: \"This solution addresses the problem\" needs context about what problem is being discussed\n",
    "    - Computational challenge: Systems must track reference and topics across multiple sentences\n",
    "  \n",
    "  - **Domain knowledge**: The meaning requires specific background knowledge\n",
    "    - Example: Understanding \"He scored a perfect 300\" requires knowing if we're discussing bowling, SAT testing, or something else\n",
    "    - Example: \"The patient presents with elevated troponin levels\" requires medical knowledge\n",
    "    - Example: \"The forward executed a perfect pick and roll\" requires basketball knowledge\n",
    "    - Computational challenge: Models need domain-specific training or specialized knowledge bases\n",
    "  \n",
    "  - **World knowledge**: The meaning requires general understanding about how the world works\n",
    "    - Example: \"The coffee is too hot to drink\" implies waiting, while \"The coffee is too cold to drink\" might imply reheating\n",
    "    - Example: \"She couldn't fit the trophy in the suitcase because it was too big\" (what was too big? requires understanding size relationships)\n",
    "    - Example: \"The child couldn't reach the cookie jar on the shelf\" requires understanding physical capabilities\n",
    "    - Computational challenge: Models need broad common-sense reasoning capabilities\n",
    "\n",
    "- **Evolving**: Language changes over time with new words and usage patterns\n",
    "  - **Neologisms**: Creation of entirely new words\n",
    "    - Example: \"Doomscrolling,\" \"NFT,\" \"cryptocurrency,\" and \"webinar\" emerged in recent years\n",
    "    - Example: \"Unfriend,\" \"selfie,\" and \"binge-watch\" entered dictionaries in the past decade\n",
    "    - Example: Technology terms like \"blog,\" \"podcast,\" and \"smartphone\" that didn't exist 30 years ago\n",
    "    - Computational challenge: Models trained on older corpora lack these terms in their vocabulary\n",
    "  \n",
    "  - **Semantic shifts**: Existing words gaining new meanings\n",
    "    - Example: \"Cloud\" shifted from weather phenomenon to data storage\n",
    "    - Example: \"Tweet\" evolved from bird sound to social media post\n",
    "    - Example: \"Viral\" changed from referring to viruses to popular online content\n",
    "    - Computational challenge: Models must distinguish between traditional and newer meanings\n",
    "  \n",
    "  - **Functional shifts**: Words changing their grammatical roles\n",
    "    - Example: \"Google\" evolved from a proper noun to a verb (\"Let me google that\")\n",
    "    - Example: \"Adult\" transformed from adjective/noun to verb (\"I can't adult today\")\n",
    "    - Example: \"Friend\" changed from noun to verb (\"Friend me on Facebook\")\n",
    "    - Computational challenge: Part-of-speech tagging systems must accommodate these changes\n",
    "  \n",
    "  - **Semantic broadening/narrowing**: Words expanding or restricting their meaning\n",
    "    - Example: \"Sick\" expanded from purely negative (ill) to also positive (excellent)\n",
    "    - Example: \"Literally\" broadened to be used for emphasis, not just \"exactly as stated\"\n",
    "    - Example: \"Gay\" narrowed from \"happy\" to specifically referring to homosexuality\n",
    "    - Computational challenge: Systems must understand both historical and contemporary usage\n",
    "\n",
    "- **Culturally influenced**: Idioms, slang, and references vary across cultures\n",
    "  - **Idioms**: Expressions whose meanings cannot be derived from their individual words\n",
    "    - Example: \"Break a leg\" is encouragement in Western theater, not a violent wish\n",
    "    - Example: \"It's raining cats and dogs\" means heavy rainfall, not animals falling from the sky\n",
    "    - Example: \"Kick the bucket\" refers to dying, not striking a container with one's foot\n",
    "    - Computational challenge: Systems must learn thousands of idiomatic expressions as special cases\n",
    "  \n",
    "  - **Regional variations**: Differences in vocabulary and usage across regions\n",
    "    - Example: American English \"elevator\" vs. British English \"lift\"\n",
    "    - Example: American English \"first floor\" is ground level, while in British English it's one level up\n",
    "    - Example: \"Table a discussion\" means to postpone it in American English but to bring it forward in British English\n",
    "    - Computational challenge: Models must recognize dialect and adjust interpretation accordingly\n",
    "  \n",
    "  - **Cultural references**: Meanings that depend on shared cultural knowledge\n",
    "    - Example: Understanding \"May the Fourth be with you\" requires familiarity with Star Wars\n",
    "    - Example: \"Don't cross the streams\" is advice about avoiding disaster (from Ghostbusters)\n",
    "    - Example: \"Winter is coming\" may convey a sense of foreboding (from Game of Thrones)\n",
    "    - Computational challenge: Systems need massive cultural knowledge bases or must identify references\n",
    "  \n",
    "  - **Slang and colloquialisms**: Informal language that varies by social group and generation\n",
    "    - Example: \"That's sick\" meaning impressive or excellent among younger generations\n",
    "    - Example: \"Throwing shade\" meaning subtle criticism or disrespect\n",
    "    - Example: \"Ghost\" as a verb meaning to suddenly cut off communication\n",
    "    - Computational challenge: Slang evolves rapidly and varies widely across demographics\n",
    "\n",
    "- **Structurally complex**: Grammar, syntax, and semantics interact in sophisticated ways\n",
    "  - **Syntactic complexity**: Sentences with intricate grammatical structures\n",
    "    - Example: Garden path sentences that mislead the reader: \"The old man the boat\" (where \"man\" is a verb)\n",
    "    - Example: \"The horse raced past the barn fell\" (The horse that was raced past the barn fell)\n",
    "    - Example: Complex embeddings: \"The rat the cat the dog chased killed ate the cheese\"\n",
    "    - Computational challenge: Parsing requires handling multiple possible grammatical interpretations\n",
    "  \n",
    "  - **Anaphora resolution**: Determining what pronouns and references point to\n",
    "    - Example: \"John told Bill that he had won the lottery\" (Who won? John or Bill?)\n",
    "    - Example: \"The trophy wouldn't fit in the suitcase because it was too big\" (What was too big?)\n",
    "    - Example: \"After the doctors treated the patients, they went home\" (Who went home?)\n",
    "    - Computational challenge: Systems must identify the correct antecedent among multiple candidates\n",
    "  \n",
    "  - **Scope ambiguity**: Uncertainty about which parts of a sentence are affected by operators like negation\n",
    "    - Example: \"I did not go to the store because it was closed\" (Did I go to the store? The reason is ambiguous)\n",
    "    - Example: \"All students read one book\" (One specific book read by all, or different books for each student?)\n",
    "    - Example: \"She only likes blue shirts\" (Only likes blue shirts and nothing else? Or likes blue shirts but no other colors?)\n",
    "    - Computational challenge: Models must determine the intended scope of quantifiers and operators\n",
    "  \n",
    "  - **Long-distance dependencies**: Relationships between words that are far apart in a sentence\n",
    "    - Example: \"The document that the lawyer who the firm hired prepared was reviewed\"\n",
    "    - Example: \"What did you say you wanted to buy?\" (connecting \"what\" with \"buy\")\n",
    "    - Example: \"This is the house that Jack built that the malt lay in that the rat ate...\"\n",
    "    - Computational challenge: Systems must track relationships across many intervening words\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
