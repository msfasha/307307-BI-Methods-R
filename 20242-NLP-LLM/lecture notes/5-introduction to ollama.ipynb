{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to Ollama\n",
        "Ollama is an open-source platform that enables users to run Large Language Models (LLMs) directly on their local machines. This approach offers enhanced data privacy, reduced latency, and greater control over AI model management, making it particularly appealing to developers, researchers, and organizations concerned with data security.\n",
        "\n",
        "**Key Features of Ollama:**\n",
        "\n",
        "1. **Local AI Model Management:** Ollama allows users to download, manage, and run various LLMs on their own hardware, eliminating the need for cloud-based services and ensuring that sensitive data remains in-house.\n",
        "\n",
        "2. **Cross-Platform Support:** Designed with versatility in mind, Ollama is compatible with macOS, Linux, and Windows operating systems, enabling a wide range of users to leverage its capabilities.\n",
        "\n",
        "3. **User-Friendly Interfaces:** Ollama offers both command-line and graphical user interfaces, catering to users with varying levels of technical expertise.\n",
        "\n",
        "4. **Extensive Model Library:** The platform provides access to a diverse collection of pre-built models, including Llama 3.3, DeepSeek-R1, Phi-4, Mistral, and Gemma 2, among others. Users can easily explore and deploy these models based on their specific requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Getting Started with Ollama:**\n",
        "\n",
        "To begin using Ollama, follow these steps:\n",
        "\n",
        "1. **Installation:**\n",
        "   - **macOS:** Download the installer from the [official website](https://ollama.com/) and follow the on-screen instructions.\n",
        "   - **Linux:** Use the package manager specific to your distribution to install Ollama. Detailed instructions are available in the [Ollama documentation](https://github.com/ollama/ollama/blob/main/docs/linux.md).\n",
        "   - **Windows:** Download the installer from the [official website](https://ollama.com/) and execute it to install Ollama.\n",
        "\n",
        "2. **Running the Ollama Service:**\n",
        "   After installation, start the Ollama service using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Couldn't find '/home/me/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is: \n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAocyQACkyelZFLtEBq9jsWYXhmFBqeLGeIcMFpWEpOJ\n",
            "\n",
            "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n"
          ]
        }
      ],
      "source": [
        "! ollama serve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This command initiates the Ollama server, making it ready to handle model requests. \n",
        "\n",
        "3. **Downloading and Running a Model:**\n",
        "   To download and run a model, such as `llama3.2`, execute:\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! ollama run llama3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the model isn't already present on your system, this command will automatically download it and start an interactive session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example Usage:**\n",
        "\n",
        "Once the service is running, you can interact with models programmatically. For instance, to generate a response using the `llama3.2` model via a REST API call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 1) (2334410403.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    curl http://localhost:11434/api/generate -d '{\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "curl http://localhost:11434/api/generate -d '{\n",
        "  \"model\": \"llama3.2\",\n",
        "  \"prompt\": \"Explain the theory of relativity.\"\n",
        "}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This command sends a prompt to the model and returns its generated response.\n",
        "**Benefits of Using Ollama:**\n",
        "\n",
        "- **Data Privacy:** Running models locally ensures that sensitive information doesn't leave your premises, addressing privacy concerns associated with cloud-based AI solutions. \n",
        "\n",
        "- **Reduced Latency:** Local execution of models leads to faster response times, which is crucial for real-time applications.\n",
        "\n",
        "- **Customization:** Users have the flexibility to fine-tune models according to their specific needs without relying on third-party services.\n",
        "\n",
        "By providing a robust platform for local AI model deployment, Ollama empowers users to harness the capabilities of advanced language models while maintaining control over their data and infrastructure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MS9vPBDDkd6"
      },
      "source": [
        "## Running Ollama on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file is based on:\n",
        "https://github.com/ed-donner/llm_engineering/blob/main/README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download and Install Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcqFHYODOBO"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3-RFec0EMwI"
      },
      "source": [
        "## When this completes..\n",
        "\n",
        "The cell above should now say: `Install complete. Run \"ollama\" from the command line.`\n",
        "\n",
        "Now click in the cell below and press Shift+Enter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j3vwuDHDwxI"
      },
      "outputs": [],
      "source": [
        "# Start Ollama server\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Start the command in a background process\n",
        "process = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# The kernel can continue execution while the process runs in the background\n",
        "print(\"The 'ollama serve' process is running in the background.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx6JAHmKEkOs"
      },
      "source": [
        "# Great!\n",
        "\n",
        "It should now say above: `The 'ollama serve' process is running in the background`.\n",
        "\n",
        "Now press Shift+Enter in each of the cells below in turn to try out Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3VSOQYiGzco"
      },
      "outputs": [],
      "source": [
        "# Download the model that we want to use - this might take a couple of mins\n",
        "# The exclamation mark at the start of the line indicates that this is a shell command, not python\n",
        "\n",
        "!ollama pull llama3.2:1b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr4iQ6alDqdf"
      },
      "outputs": [],
      "source": [
        "# install the Ollama python API package\n",
        "\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ7EoToiDqaq"
      },
      "outputs": [],
      "source": [
        "# import the package\n",
        "\n",
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA2T87uWCMKl"
      },
      "outputs": [],
      "source": [
        "# Your first message to your very own LLM running \"locally\" (well, on your cloud box)\n",
        "\n",
        "message = \"Hi there! This is my first message to an LLM!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIRqzV2jDLLJ"
      },
      "outputs": [],
      "source": [
        "# Put this message into the format expected by Ollama\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": message}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qehl86LOAuzQ"
      },
      "outputs": [],
      "source": [
        "# Let's do this!!\n",
        "\n",
        "response = ollama.chat(model=\"llama3.2:1b\", messages=messages)\n",
        "print(response['message']['content'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
