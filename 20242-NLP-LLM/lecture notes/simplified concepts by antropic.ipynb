{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Large Language Models and Transformers: A Beginner's Guide\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction\n",
    "2. Historical Context\n",
    "3. Fundamental Concepts\n",
    "4. Understanding Transformers\n",
    "5. Practical Examples\n",
    "6. Modern Applications\n",
    "7. Hands-on Activities\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Language models are computer programs that can understand, process, and generate human language. Think of them as incredibly sophisticated prediction machines that have learned patterns from vast amounts of text. Large Language Models (LLMs) are the most advanced versions of these systems.\n",
    "\n",
    "## 2. Historical Context\n",
    "\n",
    "### The Evolution of Language Processing\n",
    "\n",
    "1. Early Days (1950s-1990s)\n",
    "- Rule-based systems: Computers followed strict, hand-written rules\n",
    "- Limited capabilities: Could only handle specific tasks\n",
    "- Example: Early chatbots like ELIZA used simple pattern matching\n",
    "\n",
    "2. Statistical Revolution (1990s-2010s)\n",
    "- Shift to probability-based approaches\n",
    "- N-gram models: Predicting words based on previous words\n",
    "- Machine learning begins to emerge\n",
    "\n",
    "3. Neural Network Era (2010-2017)\n",
    "- Deep learning revolutionizes NLP\n",
    "- Word embeddings (Word2Vec, GloVe)\n",
    "- Recurrent Neural Networks (RNNs) and LSTMs\n",
    "\n",
    "4. Transformer Revolution (2017-Present)\n",
    "- Introduction of the Transformer architecture\n",
    "- Birth of models like BERT, GPT series\n",
    "- Explosion in capabilities and model sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fundamental Concepts\n",
    "\n",
    "### How Language Models Work\n",
    "\n",
    "1. **Tokenization**\n",
    "Let's understand this with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word tokenization\n",
    "sentence = \"Hello, how are you?\"\n",
    "tokens = sentence.split()\n",
    "print(tokens)  # ['Hello,', 'how', 'are', 'you?']\n",
    "\n",
    "\n",
    "# Not complete, needs working on subword tokenizations\n",
    "# More realistic subword tokenization example\n",
    "def simple_subword_tokenize(text):\n",
    "    # This is a simplified version of what real tokenizers do\n",
    "    common_subwords = ['hello', 'how', 'are', 'you', '##ing', '##ed']\n",
    "    tokens = []\n",
    "    for word in text.lower().split():\n",
    "        if word in common_subwords:\n",
    "            tokens.append(word)\n",
    "        else:\n",
    "            # Split into smaller pieces\n",
    "            tokens.append('unknown')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Context and Attention**\n",
    "Think of attention like a spotlight that helps the model focus on relevant words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention_example(query, key_value_pairs):\n",
    "    \"\"\"\n",
    "    A simplified demonstration of attention mechanism\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for key, value in key_value_pairs:\n",
    "        # Calculate similarity (greatly simplified)\n",
    "        similarity = len(set(query.split()) & set(key.split())) / len(set(query.split()))\n",
    "        scores[key] = similarity\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the weather\"\n",
    "context = [\n",
    "    (\"The weather is sunny\", \"sunny\"),\n",
    "    (\"I like pizza\", \"irrelevant\"),\n",
    "    (\"Weather forecast shows rain\", \"rain\")\n",
    "]\n",
    "\n",
    "attention_scores = simple_attention_example(query, context)\n",
    "print(\"Attention Scores:\", attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Transformers\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Self-Attention**\n",
    "The heart of the Transformer architecture. Here's a simplified demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simplified_self_attention(words):\n",
    "    \"\"\"\n",
    "    Extremely simplified version of self-attention\n",
    "    \"\"\"\n",
    "    # Create a simple similarity matrix\n",
    "    n = len(words)\n",
    "    attention_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Simple similarity: 1 if words are the same, 0.5 if one is contained in other, 0 otherwise\n",
    "            if words[i] == words[j]:\n",
    "                attention_matrix[i][j] = 1\n",
    "            elif words[i] in words[j] or words[j] in words[i]:\n",
    "                attention_matrix[i][j] = 0.5\n",
    "    \n",
    "    return attention_matrix\n",
    "\n",
    "# Example\n",
    "sentence = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "attention = simplified_self_attention(sentence)\n",
    "print(\"Attention Matrix:\")\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Position Encoding**\n",
    "How Transformers understand word order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_position_encoding(sequence_length, d_model=4):\n",
    "    \"\"\"\n",
    "    Simplified position encoding demonstration\n",
    "    \"\"\"\n",
    "    position_enc = np.zeros((sequence_length, d_model))\n",
    "    \n",
    "    for pos in range(sequence_length):\n",
    "        for i in range(d_model//2):\n",
    "            position_enc[pos, 2*i] = np.sin(pos / (10000 ** (2*i/d_model)))\n",
    "            position_enc[pos, 2*i+1] = np.cos(pos / (10000 ** (2*i/d_model)))\n",
    "    \n",
    "    return position_enc\n",
    "\n",
    "# Example\n",
    "positions = simple_position_encoding(6)\n",
    "print(\"Position Encodings for a sequence of length 6:\")\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Examples\n",
    "\n",
    "### Working with a Simple Language Model\n",
    "\n",
    "Here's a very basic example of a language model that predicts the next word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the':\n",
      "  'cat': 2\n",
      "  'mat': 3\n",
      "  'dog': 1\n",
      "  'park': 1\n",
      "  'bird': 2\n",
      "  'tree': 1\n",
      "'cat':\n",
      "  'sat': 1\n",
      "  'ran': 1\n",
      "'sat':\n",
      "  'on': 1\n",
      "'on':\n",
      "  'the': 2\n",
      "'dog':\n",
      "  'ran': 1\n",
      "'ran':\n",
      "  'in': 1\n",
      "  'on': 1\n",
      "'in':\n",
      "  'the': 1\n",
      "'bird':\n",
      "  'flew': 2\n",
      "'flew':\n",
      "  'over': 2\n",
      "'over':\n",
      "  'the': 2\n",
      "defaultdict(<class 'int'>, {'cat': 2, 'mat': 3, 'dog': 1, 'park': 1, 'bird': 2, 'tree': 1})\n",
      "mat\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class SimpleLanguageModel:\n",
    "    def __init__(self):\n",
    "        self.word_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"Train on a list of texts\"\"\"\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for i in range(len(words)-1):\n",
    "                self.word_frequencies[words[i]][words[i+1]] += 1\n",
    "        return self.word_frequencies\n",
    "    \n",
    "    def predict_next_word(self, word):\n",
    "        \"\"\"Predict the next word given the current word\"\"\"\n",
    "        if word not in self.word_frequencies:\n",
    "            return \"unknown\"\n",
    "        \n",
    "        possibilities = self.word_frequencies[word]\n",
    "        return max(possibilities.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "# Example usage\n",
    "model = SimpleLanguageModel()\n",
    "training_data = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran in the park\",\n",
    "    \"the cat ran on the mat\",\n",
    "    \"the bird flew over the tree\",\n",
    "    \"the bird flew over the mat\"\n",
    "]\n",
    "\n",
    "word_frequencies = model.train(training_data)\n",
    "\n",
    "for word, next_words in word_frequencies.items():\n",
    "    print(f\"'{word}':\")\n",
    "    for next_word, count in next_words.items():\n",
    "        print(f\"  '{next_word}': {count}\")\n",
    "\n",
    "print(model.predict_next_word(\"the\"))  # Might print 'cat' or 'dog' or 'bird'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modern Applications\n",
    "\n",
    "### Real-world Uses of LLMs:\n",
    "- Text Generation\n",
    "- Translation\n",
    "- Question Answering\n",
    "- Code Generation\n",
    "- Creative Writing\n",
    "- Data Analysis\n",
    "\n",
    "## 7. Hands-on Activities\n",
    "\n",
    "1. **Basic Text Generation Activity**\n",
    "Have students experiment with this simple text generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: dog ran in the bird flew\n"
     ]
    }
   ],
   "source": [
    "def create_word_pairs(text):\n",
    "    \"\"\"Create word pairs from text\"\"\"\n",
    "    words = text.lower().split()\n",
    "    return list(zip(words[:-1], words[1:]))\n",
    "\n",
    "def generate_text(word_pairs, start_word, length=5):\n",
    "    \"\"\"Generate text from word pairs\"\"\"\n",
    "    current_word = start_word\n",
    "    result = [current_word]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Find all possible next words\n",
    "        possible_next = [pair[1] for pair in word_pairs if pair[0] == current_word]\n",
    "        if not possible_next:\n",
    "            break\n",
    "        \n",
    "        # Choose a random next word\n",
    "        current_word = random.choice(possible_next)\n",
    "        result.append(current_word)\n",
    "    \n",
    "    return \" \".join(result)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "The cat sat on the mat.\n",
    "The dog ran in the park.\n",
    "The bird flew over the tree.\n",
    "\"\"\"\n",
    "\n",
    "word_pairs = create_word_pairs(text)\n",
    "generated_text = generate_text(word_pairs, \"the\", length=5)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Attention Visualization Activity**\n",
    "We can use this code to visualize attention patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(sentence, word_index):\n",
    "    \"\"\"\n",
    "    Visualize which words might be important for understanding a specific word\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    attention_scores = []\n",
    "    \n",
    "    # Simple attention score calculation\n",
    "    for word in words:\n",
    "        # Simple heuristic: words that often appear together get higher scores\n",
    "        score = 0.5 if word in [\"the\", \"a\", \"an\"] else 1.0\n",
    "        attention_scores.append(score)\n",
    "    \n",
    "    print(f\"When focusing on: {words[word_index]}\")\n",
    "    for word, score in zip(words, attention_scores):\n",
    "        print(f\"{word}: {'*' * int(score * 10)}\")\n",
    "\n",
    "# Example\n",
    "sentence = \"The cat sat on the mat\"\n",
    "visualize_attention(sentence, 1)  # Focus on 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This guide provides a foundation for understanding LLMs and Transformers without requiring advanced mathematics. The practical examples and activities help students grasp these concepts through hands-on experience. As students progress, they can gradually dive deeper into the mathematical concepts behind these technologies.\n",
    "\n",
    "Remember that these are simplified examples meant to illustrate concepts. Real LLMs and Transformers are much more complex but build upon these fundamental ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
