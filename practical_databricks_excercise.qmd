---
title: "Untitled"
format: html
editor: visual
---


### **Step-by-Step Guide to Azure Databricks**

#### **1. Download the Dataset**
   - **Dataset**: Use the **Online Retail Dataset** (or any CSV dataset).
   - Download Link: [UCI Online Retail Dataset](https://archive.ics.uci.edu/ml/datasets/online+retail).
   - Save the CSV file to your local machine.

---

#### **2. Set Up Azure Databricks Workspace**
   1. **Log in to Azure Portal**:
      - Go to [portal.azure.com](https://portal.azure.com) and sign in with your Azure account.
   2. **Create a Databricks Workspace**:
      - Search for "Azure Databricks" in the search bar and select it.
      - Click **Create** and fill in the required details:
        - Resource Group: Create a new or use an existing one.
        - Workspace Name: Enter a name for your workspace.
        - Pricing Tier: Use "Standard" or "Premium" (choose Standard for this session).
      - Click **Review + Create**, then click **Create** to deploy the workspace.
   3. **Launch Databricks**:
      - After deployment, navigate to your workspace and click **Launch Workspace** to open the Databricks interface.

---

#### **3. Create a Single-Machine Cluster**
   1. In Databricks, go to the **Compute** tab in the left sidebar.
   2. Click **Create Cluster** and configure it:
      - **Cluster Name**: "Student Cluster."
      - **Cluster Mode**: Single Node.
      - **Node Type**: Select a lightweight machine, e.g., Standard_DS3_v2.
      - **Runtime Version**: Use the latest Databricks LTS version.
   3. Click **Create Cluster** and wait for it to start.

---

#### **4. Upload Dataset to Azure Blob Storage**
   1. **Set Up Blob Storage**:
      - In the Azure portal, search for **Storage Accounts** and create a new storage account.
        - Resource Group: Same as the Databricks workspace.
        - Storage Account Name: "retaildata123" (example).
        - Performance: Standard.
        - Redundancy: Locally-redundant storage (LRS).
      - Click **Review + Create**, then **Create**.
   2. **Upload the Dataset**:
      - Go to your storage account and click on **Containers**.
      - Create a new container (e.g., "datasets") and set its access level to **Private**.
      - Open the container and upload the downloaded CSV file (e.g., "online_retail.csv").

---

#### **5. Connect Databricks to Blob Storage**
   1. In Databricks, go to your notebook.
   2. Use the following Python code to connect to Blob Storage:
      ```python
      storage_account_name = "retaildata123"
      container_name = "datasets"
      account_key = "<your_account_key>"

      spark.conf.set(
          f"fs.azure.account.key.{storage_account_name}.blob.core.windows.net",
          account_key
      )
      data_path = f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/online_retail.csv"
      ```
      - Replace `<your_account_key>` with the key from the Azure Storage account:
        - Go to the storage account -> **Access Keys** -> Copy Key1.

---

#### **6. Load and Explore Data**
   1. Use the following code to load the dataset:
      ```python
      df = spark.read.csv(data_path, header=True, inferSchema=True)
      df.show(5)
      ```
   2. Explore the data:
      - Print schema:
        ```python
        df.printSchema()
        ```
      - Count rows:
        ```python
        df.count()
        ```

---

#### **7. Perform Data Transformations**
   - Example: Clean the data by filtering rows where quantity is positive.
     ```python
     clean_df = df.filter(df.Quantity > 0)
     clean_df.show(5)
     ```

   - Create a new calculated column:
     ```python
     from pyspark.sql.functions import col
     clean_df = clean_df.withColumn("TotalPrice", col("Quantity") * col("UnitPrice"))
     clean_df.show(5)
     ```

---

#### **8. Query the Data Using SQL**
   1. Register the DataFrame as a temporary SQL table:
      ```python
      clean_df.createOrReplaceTempView("retail_data")
      ```
   2. Perform SQL queries:
      ```sql
      %sql
      SELECT Country, SUM(TotalPrice) AS TotalRevenue
      FROM retail_data
      GROUP BY Country
      ORDER BY TotalRevenue DESC
      ```

---

#### **9. Visualize Results**
   - Click on the chart icon in the query results to create visualizations:
     - **Bar Chart**: Total Revenue by Country.
     - **Pie Chart**: Revenue distribution.

---

#### **10. Save the Processed Data**
   - Save the transformed dataset back to DBFS as a Parquet file:
     ```python
     clean_df.write.format("parquet").save("/FileStore/tables/cleaned_retail_data.parquet")
     ```

---

#### **11. Work with SQL Warehouse**
   1. Go to the **SQL** section in the Databricks UI.
   2. Create a new **SQL Warehouse** and attach it to your workspace.
   3. Query the saved Parquet file:
      ```sql
      SELECT * FROM parquet.`/FileStore/tables/cleaned_retail_data.parquet`
      ```

---

#### **12. Wrap-Up**
   - Recap:
     - Ingest data from Azure Blob Storage.
     - Clean and transform data using Spark.
     - Query data using SQL.
     - Save processed data for further analysis.

---